{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Dynamic Objective Surface to Tackle Harder Problems to Optimize\n",
    "\n",
    "<center><img src=images/erm_2.png width=35%></center>\n",
    "\n",
    "Everything is base back to **Empirical Risk Minimization** or the equivalence of it for objective functions instead of loss functions. It is the most foundationaal modern unsconstrained optimization method and all neural network, reinforcement learning algorithm, etc... is just building more and more fancy ways, more complicated, and more intelligent ways of finding such optimization point (using memory, using system, ...) on the objective function's surface, all just some sort of **unconstrained global search optimization**.\n",
    "\n",
    "<center><img src=images/erm_1.png width=30%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Main Training Loop\n",
    "Demistifying TorchRL, all components is customizable and adjustable\n",
    "\n",
    "The main training loop for TorchRL is very very simple and direct, cna be directly transformed from the PPO paper\n",
    "- Step MDP\n",
    "- Collect radnom sample data using **probabilistic policy player** with DataCollector -> ReplayBuffer\n",
    "    - Compute rewards via **TD bellman equation**\n",
    "    - Compute advantage (GAE) via comparison using ReplayBuffer data\n",
    "        - Loop over the collected to compute loss values\n",
    "        - Back propagate (SGD on value using MSE (Adam)) -> better GAE understanding what is good\n",
    "        - Optimize policy by maximize ppo_clip objective (uses GAE) -> going to the direction that GAE points at (Adam)\n",
    "            - Forward pass: calculate loss\n",
    "            - Backward pass: SGD update network and zero out gradient\n",
    "            - To preserve creativity with alignment: PPO Loss = Value Loss + Policy Loss + Entropy Loss\n",
    "        - Repeat\n",
    "    - Repeat\n",
    "- Repeat\n",
    "\n",
    "Algorithm link: https://pytorch.org/rl/tutorials/coding_ppo.html\n",
    "\n",
    "<center><img src=images/ppo.png width=70%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Loss Functions & Full Algorithm Representation:\n",
    "PPO Algoriithm: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "This is the full representation of the PPO Algorithm\n",
    "<center><img src=images/ppo_3.svg width=60%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network Optimization\n",
    "This representation is an **off-policy** version of ppo in some sense, uisng **`important sampling ratio`**, **`clipping`**, and **`KL Divergence`** to make a more preservative approach on making updates:\n",
    "\n",
    "1. The **important sampling ratio** $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$ compares the action taken by the current policy and the action taken by the previous policy, which indicates how much more or less likely the new policy is to take action $a$ under state $s_t$ compared to the previosu policy, which **bounds the action from been too drastic change**.\n",
    "\n",
    "2. $g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))$ is a clipped version of this ratio, which uses the advantage function $A^{\\pi_{\\theta_k}}(s,a)$ and a clipping parameter $\\epsilon$ to **prevent the new policy from moving too far away from the old policy**.\n",
    "    - We are taking the $min$ of it, showing a more **preservative** approach.\n",
    "\n",
    "3. The $D_k$ value signifies **KL Divergence** that measures ho big of a change did the update cause on the policy (statistical method) -> bigger change have more punishments.\n",
    "\n",
    "4. Reacall that in the original **policy gradient** (gradient search method), it is about maximizing the **expectation of the reward**. The Expectation here is formed by doing empirical sample mean over a finite batch of trajectories collected under the previous policy from the replay buffer.\n",
    "\n",
    "5. Think in terms of an optimization problem now, **`abstract the problem now to a search problem`**, we are trying to find maximum point of a non-convex plain formed by this empirical loss function, now update the policy network's parameter in the direction of **stochastic distribution gradient ascent**, but the actual update of parameter is in **back proporgation**.\n",
    "    - ***Training a Neural Network itself is an optimization problem***\n",
    "    - Neural network is essentially a bunch of **weights**, updgraded linear rgression or perceptron, these weights having a non-linear relationship with objective function forming a high-dimensional risk surface using the objective function, then using gradient search to do unconstrained optimization on this surface via **gradient search**.\n",
    "    - The objective function of risk function is guided **dynamically** with the advantageous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network Gradient Decent\n",
    "On the other hand, the value network is updated via **`Mean Square Error`**:\n",
    "\n",
    "1. Comparing randomly **new** sampled reward with **different path computed by current policy** at the **current state** (that try to get reward to the best by default **max bellman update** idea) with the current known reward at current state:\n",
    "\n",
    "    - ***One interpretation***: from looking at the setting of bellman update, here is where the assumption of bellman equation comes in, bellman update is always assuming getting the better next state, so compare current state reward with randomly sampled **theoritically best current** (new sample with $max(Q(s,a))$) makes sense. **Bellman update ganrantees helping the agent getting more reward when updating like this -> help to expand understanding of the MDP**\n",
    "\n",
    "    - ***Second interpretation***: from a more search perspective, this is **expanding the understanding of the MDP**, keep trying to reshape the value neural network's **\"memory\"** towards the direction that have higher reward -> some what like a **learning or adapted heuristic** idea.\n",
    "\n",
    "2. The $D_k$ value signifies **KL Divergence** that measures ho big of a change did the update cause on the value (statistical method) -> bigger change have more punishments.\n",
    "\n",
    "3. We try to minimize this value, usually with ADAM optimizer, which is still **stochastic gradient acsent**.\n",
    "    - Again, think in terms of an optimization problem now, **`abstract the problem now to a search problem`**, we are trying to find maximum point of a non-convex plain formed by this empirical loss function.\n",
    "    - The actual update of parameter is in **back proporgation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning With Brax\n",
    "- Repository from Charles: https://github.com/charles-zhng/Brax-Imitation\n",
    "- Repository from Talmo's Lab: https://github.com/talmolab/VNL-Brax-Imitation\n",
    "- MOCAP Data Set: https://drive.google.com/file/d/10WbPKUr9_1vH0c5KwuqpvdIlcRGEeE2k/view?usp=drive_link\n",
    "\n",
    "<center><img src=images/imitation_pipeline_7.png width=90%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brax interaction + network training\n",
    "<center><img src=images/brax_env_imitation_4.png width=70%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ppo train detailed breakdown\n",
    "<center><img src=images/ppo_train_breakdown_3.png width=70%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ppo train hyperparameter tuning\n",
    "<center><img src=images/ppo_hyperparameter.png width=70%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brax environment function in relation to the networks\n",
    "<center><img src=images/brax_function_2.png width=60%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brax-imitation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
